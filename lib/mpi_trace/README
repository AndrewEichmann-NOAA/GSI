This directory has libraries that can be used to track the time
spent in MPI routines.  There are two basic methods:

(1) Link your application with one of the static libraries
    described below (example: libmpitrace.a), then run the job.

(2) Set MP_EUILIBPATH to a directory that contains an instrumented 
    version of libmpi_r.a, then run the job.

The second method does not require any special steps for compilation 
or linking, and it is the preferred way to get MPI timing information
when the executable is stripped or when there are MPI calls in shared
libraries, such as PESSL.  

In either case, the idea is intercept MPI calls, using the profiling
entry points provided by the MPI library.  The static libraries are
described next.  For method (2), the static timing libraries are used
to construct a modified MPI shared-library, libmpi_r.a; and so the   
same features are available for both methods.
    

There are two main static timing libraries:

libmpitrace.a : wrappers for low-overhead MPI elapsed-time measurements

libmpihpm.a   : the wrappers above plus hpm counter data


You can only use one of these libraries at a time.


To build these files, just type "make" in the src directory.  The 
libraries above should contain both 32-bit and 64-bit code; so they 
should work with either 32-bit or 64-bit apps.  

The default mode is to just collect timing summary data, which is
printed when the application calls MPI_Finalize().  You can also save
time-stamped records for graphical display - the details are sketched
below in the section on "Event Tracing".

There are also options to control the binding of MPI processes and
threads.  These are discussed in the section on "Binding".


To use the basic wrappers, link with libmpitrace.a, and then run the
application.  By default you will get text files mpi_profile.0,
mpi_profile.1, mpi_profile.2, ... etc.  The output is written when the
application calls MPI_Finalize().  Instead of generating one small text
file from each MPI rank, there is an option to dramatically reduce the
number of output files.  At the present time, this option can be
enabled by setting an env variable SAVE_ALL_TASKS=no.  With this
option, detailed data is saved for MPI rank 0, and for the ranks with
the minimum, maximum, and median communication times.  The text file
for rank 0 contains a summary for all other MPI ranks.  It is hoped
that this option will provide sufficient information, while keeping 
the number of files manageable for very large task counts.


To use libmpihpm.a, link with libmpihpm.a -lpmapi -lcfg -lodm.  Choose
a performance counter group (for example group 0):

export HPM_GROUP=0

then run the code.  On power5 there are shared counters, which make
things more complicated when SMT is enabled.  To handle that case, it
is necessary to bind MPI tasks to processors, which you can do using
any of several methods.  There is some support for binding in the 
MPI wrapper libraries, by setting an environment variable:

export BIND_TASKS=yes

This binding method is less than ideal for power5 systems, because it
binds after the application is started, and that can result in static
data that is not local.  It is better to pre-bind, so Farid Parpia's
"launch" method or a comparable helper script is often the preferred
method for binding.

Power5/Power5+ counter groups are listed in power5.ref and 
power5+.ref.  By default you will get counter group 0.  The counters 
are started in MPI_Init(), and stopped in MPI_Finalize().  The current
default output method is a small text file for each MPI rank, for 
example mpi_profile_group78.10 would have data for counter-group 78 
from MPI rank 10.  There is an option to consolidate the data, and 
reduce the number of output files, by setting "SAVE_ALL_TASKS=no".

You can specify a list of counter groups:

export HPM_GROUP_LIST="0,43,44,78,79"

In this case task 0 has group 0, task 1 has group 43, etc. If SMT is
enabled, one has to ensure that both tasks on the same physical cpu
are using the same counter group.  When SMT is enabled and there are
more MPI processes than physical processors, the library will attempt
to bind successive MPI ranks to successive logical cpus. In this case,
with the group list above, the library would use group 0 for MPI ranks
0 and 1, group 43 for MPI ranks 2 and 3, etc.  You can also force this
pair-wise method of assigning groups if you set the env variable
SMT_PACKED=yes.


In summary mode, the time spent in MPI routines is accumulated starting
in MPI_Init() and stopping in MPI_Finalize().  In some cases you may
want to collect MPI timing data for only a certain interval.  You can
do this by calling routines to start/stop the collection of summary
statistics:

Fortran syntax:
        call summary_start()
        do work + MPI ...
        call summary_stop()

C syntax:
  summary_start();
  do work + MPI ...
  summary_stop();

If you control summary statistics in this way, the accumulated data and
timers are reset when the first call to summary_start() is made, and
the output files should contain MPI timing data just for the interval
that you specified.

For MPI analysis it can be useful to distinguish between different
calls to the same MPI routine based, for example, on the instruction
address, or location in the code.  There are several ways to get such
data.  With any of the libraries, you can set an environment variable:

export PROFILE_BY_CALL_SITE=yes

This will produce a summary of elapsed time tied to the instruction
address of the MPI call in the application. You can use this to find
out which MPI_Recv call takes the most time, for example.  To use this
feature, make sure that you use -g when you compile and link, and also 
specify the -bnoobjreorder linker option.  Then you can use the GNU 
addr2line utility to translate from instruction address in the output 
to source-file and line number.  You can set another env variable,
TRACEBACK_LEVEL, to control how far up the call stack you want to go.
This is useful for applications that have their own communication
layer.  It turns out that building a modified libmpi_r.a adds one to
the call-stack, and so if you use the MP_EUILIBPATH method for profiling,
you should add 1 to the value of TRACEBACK_LEVEL.

An alternative method to associate MPI time with regions in the code is
provided by setting PROFILE_BY_SUBROUTINE=yes.  To use this method, add
"-qtbtable=full" or "-g" as an additional compiler option.  This
approach uses a trace-back method to find the name of the routine that 
called the MPI function, and this only works if there is a full 
trace-back table.  This method associates time in MPI routines by 
subroutine name in the user's application, instead of by function address.
The profile-by-address method mentioned above is more specific, but 
requires an additional step to translate from instruction addresses to 
source files and line numbers, so the PROFILE_BY_SUBROUTINE method is
more convenient.  You can also set TRACEBACK_LEVEL as described above,
to control the location up the call-stack that you want to use for
profiling.

There is a TRACE_DIR environment variable that can be used to specify 
the directory for mpi trace files - the default is to write the files 
in the working directory for each MPI task.  

---------------------------------------------------------------------

Binding:

For all of the versions you can choose to bind MPI tasks to processors
if you set an environment variable BIND_TASKS to yes.  You can 
optionally set the variables BIND_BASE and BIND_INC to control the 
placement of tasks on cpus.  Process binding starts at the logical cpu
indicated by BIND_BASE (default is zero) and increments by BIND_INC
(defaults to one or two, depending on the SMT status and number of
tasks per node).  If you want to control binding for mixed MPI+OpenMP 
applications, you can use the _smp version of the wrapper libraries:
libmpitrace_smp.a, libmpihpm_smp.a, libmpiprof_smp.a.  The only
difference between these "smp" libraries and the standard ones is that
the smp libraries use OpenMP to create a parallel region, and then bind 
each thread to a processor, inside the wrapper for MPI_Init.  However, 
since binding is done inside the application after it has started, the
static data can be non-local; and binding is only done once, when the 
job starts. One should use OMP_NUM_THREADS or the equivalent XLSMPOPTS 
to control the number of threads per MPI process.  If the application 
calls set_omp_num_threads() to dynamically set the number of threads,
then this method of binding will not work, and one would have to make
the equivalent calls after the number of threads is set.  The binding 
method has been constructed so that it should work correctly even for 
MPMD jobs, where some executables are single-threaded and others are 
multi-threaded.  In that case, one would link single-threaded 
executables with libmpitrace.a and link multi-threaded executables 
with libmpitrace_smp.a; set OMP_NUM_THREADS appropriately and run
the job using BIND_TASKS=yes.

---------------------------------------------------------------------

The main objective for libmpitrace.a is to provide a very low overhead
elapsed-time measurement of MPI routines for applications written in
any mixture of Fortran, C, and C++.  The overhead for the current
version is less than 1 microsecond per call.  The read_real_time()
routine is used to measure elapsed-time, with a direct method to
convert timebase structures into seconds.  This is much faster than
using rtc() or the time_base_to_time() conversion routine.


---------------------------------------------------------------------

Event Tracing:

You can optionally save a time-stamped record of MPI events for
graphical display - similar to other MPI tracing tools, such as
jumpshot, vampirtrace, etc.  You can trace from the beginning by
setting TRACE_ALL_EVENTS=yes, or you can trace specific code sections
by adding calls to trace_start() and trace_stop() to your application.
If you use the trace_start()/trace_stop() method, which is recommended,
you should not set TRACE_ALL_EVENTS=yes, because that starts tracing
immediately after MPI_Init.  The wrappers reserve a small memory buffer
to accumulate trace data.  Once this buffer is full, additional
time-stamped records are not saved.  When tracing is enabled, a single
binary trace file will be written during MPI_Finalize().  This file is
named "events.trc", and can be graphically explored using "traceview".
The main idea is to provide a graphical picture of the time-sequence of
MPI calls, while providing a link from each MPI event to the source
code.  Each MPI event contains the parent and grandparent instruction
address.  The easiest way to get source-code association is to use the
GNU addr2line utility:

addr2line -e your.x hex_instruction_address 2>/dev/null

You can download and build the latest GNU bin utilities for both 32 and
64-bit binaries to get addr2line.  The addr2line utility can translate
from address to line number and source file if you use -g when you
compile and link.  You also have to specify the -bnoobjreorder linker
option, else the addr2line utility can't interpret the xcoff binary.
You can still visualize the trace file without these options, but you
need them to correctly associate instruction addresses with the source
code.


At the present time, the visualization tool "traceview" can be built to
run on most platforms, including big and little endian.  This tool is
meant to run on your local workstation, using capable OpenGL graphics
hardware and software.  The trace visualization tool "traceview" is 
described elsewhere.

---------------------------------------------------------------------

Note: The Fortran trace wrappers are in lower case (mpi_send).  If the
Fortran source is compiled with the -qmixed option, then the names of
MPI routines are not mapped to lower case, and you have to make sure
that the wrappers have names that match the case used in the
application.

---------------------------------------------------------------------

Note: the current version is not thread-safe, so it should be used in
single-threaded applications, or when only one thread makes MPI calls.
The wrappers could be made thread-safe by adding mutex locks around
updates of static data - which would add some additional overhead.

---------------------------------------------------------------------

Recent fixes/features
 (1) Fixed the filename for output files.  Older versions could fail
     in applications that use multiple communicators and task
     reordering.

 (2) All Fortran wrappers now have the Fortran profiling interface, so
     codes that use MPI_BOTTOM should run correctly.

 (3) Both 32-bit and 64-bit objects are packaged into the trace
     libraries.  Must build the 64-bit library for the target AIX
     level.

 (4) Output from power-4 counters has been added (for AIX 5 and
     power-4 only) with libmpihpm.a

 (5) May 2002: The option to bind tasks to processors was added.

 (6) Nov 2002: Modified wrappers for collective MPI routines
     so they should work with MPI_IN_PLACE.  Changed mpcc to mpcc_r in
     the makefile to get the MPI-2 features in the threaded library.

 (7) Dec 2002: Added wrappers for MPI_Iprobe, MPI_Test, MPI_Testany,
     MPI_Testall, MPI_Testsome.  Changed event counters to 64-bit
     integers to prevent counter overflow.

 (8) March 2003 : added TRACE_DIR env variable to direct trace files
     to a user-specified directory.  Added both standard and extname
     Fortran entry points into each library by default.

 (9) March 2004 : added HPM_GROUP_LIST environment variable to get
     hardware counter data for a list of counter groups in one job.

(10) March 2004 : added the ability to record a binary trace file for
     graphical display.

(11) July 2004 : bug fixed in wrappers for MPI collective routines

(12) August 2004 : wrappers updated for power5 counters

(13) January 2006 : added power5+, changed output to save data from
     the MPI ranks with the min, max, and median comm times when the
     env variable SAVE_ALL_TASKS=no. Added smp binding method for mixed
     OpenMP+MPI (requires -qsmp link option).

(14) February 2006 : added a feature to associate elapsed time in MPI
     routines with the instruction address of the call.  To enable this
     feature, compile with -g, and use linker options -g and
     -bnoobjreorder; and set an env variable PROFILE_BY_CALLER=yes.

(15) April 5, 2006.  Added an option to accumulate data over an
     interval specified by the user instead of from MPI_Init() to
     MPI_Finalize().  To use this add calls to summary_start() and
     summary_stop(). Merged code for binding either pure MPI or mixed
     OpenMP + MPI applications.  To bind midex-mode applications, link
     with libmpitrace_smp.a, or one of the other "smp" library versions.

(16) August 2006.  Added an option to separately account for on-node
     communication, by setting PROFILE_BY_NODE=yes.  This adds some
     overhead, because each message has to be checked to find out if
     the source or destination ranks are on the same node.  There are
     a number of limitations due to the nature of MPI, so in general
     one will not catch all on-node communication by this method.

(17) August 2006.  Added a method to re-build the underlying MPI
     library with instrumented entry points.  This method does not
     require linking with any special library, and it should work
     for stripped executables and MPI calls in shared-libraries.
     To use this method, it is only necessary to set MP_EUILIBPATH
     to point to the modified libmpi_r.a.

(18) February 2007.  Added wrappers for MPI_Init_thread().

(19) March 2007.  Fixed errors in handling 64-bit addresses.  The
     trace file format saves instruction addresses as 32-bit ints;
     AIX 64-bit mode uses an offset of 0x100000000 for program
     text, and so this offset has been added to get the correct
     64-bit addresses.

(20) March 2007.  Moved the profile-by-subroutine feature into
     libmpitrace.a and libmpihpm.a, eliminating the need for a 
     third library.   Set PROFILE_BY_SUBROUTINE=yes to enable this
     feature.  Changed the name of the environment variable to 
     profile by call site: was PROFILE_BY_CALLER=yes, now is
     PROFILE_BY_CALL_SITE=yes.  Change was made to make a more
     clear distinction between profiling by subroutine name vs.
     instrution address of the call-site. Fixed the code for
     PROFILE_BY_CALL_SITE to work in the same way as the method
     for PROFILE_BY_SUBROUTINE.


Please send corrections/suggestions to walkup@us.ibm.com.
